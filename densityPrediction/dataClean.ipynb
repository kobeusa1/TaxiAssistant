{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "aa = rdd.map(lambda x: sys.version)\n",
    "print aa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/hongyili/Untitled Folder\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/Users/hongyili/Untitled Folder')\n",
    "import geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_rdd = sc.textFile(\"/Users/hongyili/Downloads/yellow*.csv\")\n",
    "y_rdd = y_rdd.map(lambda line: tuple(line.split(',')))\n",
    "\n",
    "g_rdd = sc.textFile(\"/Users/hongyili/Downloads/green*.csv\")\n",
    "g_rdd = g_rdd.map(lambda line: tuple(line.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "import math\n",
    "\n",
    "def date_extractor(date_str,b,minutes_per_bin):\n",
    "    # Takes a datetime object as a parameter\n",
    "    # and extracts and returns a tuple of the form: (as per the data specification)\n",
    "    # (time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend)\n",
    "    # Split date string into list of date, time\n",
    "    \n",
    "    d = date_str.split()\n",
    "    \n",
    "    #safety check\n",
    "    if len(d) != 2:\n",
    "        return tuple([None,])\n",
    "    \n",
    "    # TIME (eg. for 16:56:20 and 15 mins per bin)\n",
    "    #list of hour,min,sec (e.g. [16,56,20])\n",
    "    time_list = [int(t) for t in d[1].split(':')]\n",
    "    \n",
    "    #safety check\n",
    "    if len(time_list) != 3:\n",
    "        return tuple([None,])\n",
    "    \n",
    "    # calculate number of minute into the day (eg. 1016)\n",
    "    num_minutes = time_list[0] * 60 + time_list[1]\n",
    "    \n",
    "    # Time of the start of the bin\n",
    "    time_bin = num_minutes / minutes_per_bin     # eg. 1005\n",
    "    hour_bin = num_minutes / 60                  # eg. 16\n",
    "    min_bin = (time_bin * minutes_per_bin) % 60  # eg. 45\n",
    "    \n",
    "    #get time_cat\n",
    "    hour_str = str(hour_bin) if hour_bin / 10 > 0 else \"0\" + str(hour_bin)  # eg. \"16\"\n",
    "    min_str = str(min_bin) if min_bin / 10 > 0 else \"0\" + str(min_bin)      # eg. \"45\"\n",
    "    time_cat = hour_str + \":\" + min_str                                     # eg. \"16:45\"\n",
    "    \n",
    "    # Get a floating point representation of the center of the time bin\n",
    "    time_num = (hour_bin*60 + min_bin + minutes_per_bin / 2.0)/(60*24)      # eg. 0.7065972222222222\n",
    "    \n",
    "    time_cos = math.cos(time_num * 2 * math.pi)\n",
    "    time_sin = math.sin(time_num * 2 * math.pi)\n",
    "    \n",
    "    # DATE\n",
    "    # Parse year, month, day\n",
    "    date_list = d[0].split('-')\n",
    "    d_obj = date(int(date_list[0]),int(date_list[1]),int(date_list[2]))\n",
    "    day_to_str = {0: \"Monday\",\n",
    "                  1: \"Tuesday\",\n",
    "                  2: \"Wednesday\",\n",
    "                  3: \"Thursday\",\n",
    "                  4: \"Friday\",\n",
    "                  5: \"Saturday\",\n",
    "                  6: \"Sunday\"}\n",
    "    day_of_week = d_obj.weekday()\n",
    "    day_cat = day_to_str[day_of_week]\n",
    "    day_num = (day_of_week + time_num)/7.0\n",
    "    day_cos = math.cos(day_num * 2 * math.pi)\n",
    "    day_sin = math.sin(day_num * 2 * math.pi)\n",
    "    \n",
    "    year = d_obj.year\n",
    "    month = d_obj.month\n",
    "    day = d_obj.day\n",
    "    \n",
    "    weekend = 0\n",
    "    #check if it is the weekend\n",
    "    if day_of_week in [5,6]:\n",
    "        weekend = 1\n",
    "       \n",
    "    return (year, month, day, time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_cleaner(zipped_row):\n",
    "    # takes a tuple (row,g,b,minutes_per_bin) as a parameter and returns a tuple of the form:\n",
    "    # (time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend,geohash)\n",
    "    row = zipped_row[0]\n",
    "    g = zipped_row[1]\n",
    "    b = zipped_row[2]\n",
    "    minutes_per_bin = zipped_row[3]\n",
    "    # The indices of pickup datetime, longitude, and latitude respectively\n",
    "    indices = (1, 6, 5)\n",
    "    \n",
    "    #safety check: make sure row has enough features\n",
    "    if len(row) < 7:\n",
    "        return None\n",
    "    \n",
    "    #extract day of the week and hour\n",
    "    date_str = row[indices[0]]\n",
    "    clean_date = date_extractor(date_str,b,minutes_per_bin)\n",
    "    #get geo hash\n",
    "\n",
    "    latitude = float(row[indices[1]])\n",
    "    longitude = float(row[indices[2]])\n",
    "    location = None\n",
    "    #safety check: make sure latitude and longitude are valid\n",
    "    if latitude < 41.1 and latitude > 40.5 and longitude < -73.6 and longitude > -74.1:\n",
    "        location = geohash.encode(latitude,longitude, g)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return tuple(list(clean_date)+[location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = 7 #geohash length\n",
    "b = 48 # number of time bins per day\n",
    "# Note: b must evenly divide 60\n",
    "minutes_per_bin = int((24 / float(b)) * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gclean_rdd = g_rdd.map(lambda row: (row, g, b, minutes_per_bin)).map(data_cleaner).filter(lambda row: row != None).map(lambda row: (row,1)).reduceByKey(lambda a,b: a + b).map(lambda row: (row,'g'))          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yclean_rdd = y_rdd.map(lambda row: (row, g, b, minutes_per_bin)).map(data_cleaner).filter(lambda row: row != None).map(lambda row: (row,1)).reduceByKey(lambda a,b: a + b).map(lambda row: (row,'y')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "combined_rdd = yclean_rdd.union(gclean_rdd)\n",
    "final_rdd = combined_rdd.map(lambda row: row[0]).reduceByKey(lambda a,b: a + b).map(lambda (a,b): (a,b,np.random.random()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fraction = 0.1\n",
    "valid_fraction = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainset = final_rdd.filter(lambda (a,b,c): ((a[0] == 2015) | (a[1] == 3)) & (c <= train_fraction))\n",
    "trainset.repartition(1).saveAsTextFile(\"/Users/hongyili/Desktop/big_Data_final／trainset11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validset = final_rdd.filter(lambda (a,b,c): (a[0] == 2015) & (a[1] == 4) & (c <= valid_fraction))\n",
    "validset.repartition(1).saveAsTextFile(\"/Users/hongyili/Desktop/big_Data_final／trainset6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset  = final_rdd.filter(lambda (a,b,c): (a[0] == 2015) & (a[1] == 5)) \n",
    "testset.repartition(1).saveAsTextFile(\"/Users/hongyili/Desktop/big_Data_final/trainset6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import geohash\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None, verbose=0):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func, verbose=verbose)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds, verbose=verbose)\n",
    "    gs.fit(X, y)\n",
    "    print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_, gs.scorer_\n",
    "    print \"Best score: \", gs.best_score_\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868979, 11)\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\"time_cat\", \"time_num\", \"time_cos\", \"time_sin\", \"day_cat\", \"day_num\", \"day_cos\", \"day_sin\", \"weekend\", \"geohash\", \"pickups\"]\n",
    "dftaxi=pd.read_csv(\"/Users/hongyili/Desktop/big_Data_final/train_csv/train.csv\", header=None, names = names)\n",
    "print dftaxi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itrain, itest = train_test_split(xrange(dftaxi.shape[0]), train_size=0.8)\n",
    "mask=np.ones(dftaxi.shape[0], dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)\n",
    "mask[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ True False  True ...,  True  True  True]\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnames = [\"time_cat\", \"time_num\", \"time_cos\", \"time_sin\", \"day_cat\",\n",
    "          \"day_num\", \"day_cos\", \"day_sin\", \"weekend\", \"geohash\"]\n",
    "X = dftaxi[Xnames]\n",
    "\n",
    "y = np.log10(dftaxi['pickups']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decodegeo(geo, which):\n",
    "    if len(geo) >= 6:\n",
    "        geodecoded = geohash.decode(geo)\n",
    "        return geodecoded[which]\n",
    "    else:\n",
    "        return 0\n",
    "X['latitude'] = X['geohash'].apply(lambda geo: decodegeo(geo, 0))\n",
    "X['longitude'] = X['geohash'].apply(lambda geo: decodegeo(geo, 1))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          time_cat  time_num  time_cos  time_sin    day_cat   day_num   day_cos   day_sin  weekend  geohash   latitude  longitude\n",
       "2015 4 28    14:30  0.614583 -0.751840 -0.659346    Tuesday  0.230655  0.121251  0.992622        0  dr5ru54  40.754471 -74.000473\n",
       "     3 19    19:30  0.822917  0.442289 -0.896873   Thursday  0.546131 -0.958287 -0.285808        0  dr5rut6  40.766830 -73.978500\n",
       "     5 16    00:30  0.031250  0.980785  0.195090   Saturday  0.718750 -0.195090 -0.980785        1  dr5ryj1  40.765457 -73.913956\n",
       "     3 8     04:00  0.177083  0.442289  0.896873     Sunday  0.882440  0.739379 -0.673289        1  dr5rv38  40.746231 -73.948288\n",
       "     4 15    03:30  0.156250  0.555570  0.831470  Wednesday  0.308036 -0.356622  0.934249        0  dr5rgcx  40.746231 -74.004593\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.drop(['time_cat','day_cat','geohash'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695183, 9)\n",
       "(695183, 9)\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "n_samples = Xtrain.shape[0]\n",
    "n_features = Xtrain.shape[1]\n",
    "print Xtrain.shape\n",
    "max_samples = 1000000\n",
    "if Xtrain.shape[0] > max_samples:\n",
    "    rows = random.sample(Xtrain.index, max_samples)\n",
    "    Xtrain = Xtrain.ix[rows]\n",
    "    ytrain = ytrain.ix[rows]\n",
    "print Xtrain.shape\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor(n_estimators=20, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
       "[CV] max_features=auto, n_estimators=50, max_depth=50 ................\n",
       "[CV]  max_features=auto, n_estimators=50, max_depth=50, score=-0.032575 - 2.0min\n",
       "[CV] max_features=auto, n_estimators=50, max_depth=50 ................\n",
       "[CV]  max_features=auto, n_estimators=50, max_depth=50, score=-0.032608 - 1.9min\n",
       "[CV] max_features=auto, n_estimators=50, max_depth=50 ................\n",
       "[CV]  max_features=auto, n_estimators=50, max_depth=50, score=-0.032664 - 1.8min\n",
       "[CV] max_features=auto, n_estimators=50, max_depth=50 ................\n",
       "[CV]  max_features=auto, n_estimators=50, max_depth=50, score=-0.032964 - 1.9min\n",
       "[CV] max_features=auto, n_estimators=50, max_depth=50 ................\n",
       "[CV]  max_features=auto, n_estimators=50, max_depth=50, score=-0.032654 - 1.7min\n",
       "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  9.2min finished\n",
       "BEST {'max_features': 'auto', 'n_estimators': 50, 'max_depth': 50} -0.0326930578467 [mean: -0.03269, std: 0.00014, params: {'max_features': 'auto', 'n_estimators': 50, 'max_depth': 50}] make_scorer(mean_squared_error, greater_is_better=False)\n",
       "Best score:  -0.0326930578467\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\"n_estimators\": [50],\n",
    "              \"max_features\": [\"auto\"], # [\"auto\",\"sqrt\",\"log2\"]\n",
    "              \"max_depth\": [50]}\n",
    "best = cv_optimize(estimator, parameters, Xtrain, ytrain, n_folds=5, score_func='mean_squared_error', verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg=best.fit(Xtrain, ytrain)\n",
    "training_accuracy = reg.score(Xtrain, ytrain)\n",
    "test_accuracy = reg.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.713449438461\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.round(np.power(10,np.column_stack((reg.predict(Xtest),ytest))) - 1,decimals=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(reg.predict(Xtest),ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "dict_feat_imp = dict(zip(list(X.columns.values),reg.feature_importances_))\n",
    "\n",
    "sorted_features = sorted(dict_feat_imp.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_cols = list(X.columns.values)\n",
    "time_cols.remove('latitude')\n",
    "time_cols.remove('longitude')\n",
    "loc_cols = ['latitude', 'longitude']\n",
    "time_data = X.drop(loc_cols, axis=1).drop_duplicates()\n",
    "time_data = time_data[time_data['day_num'] <= 1/7.]\n",
    "loc_data = X.drop(time_cols, axis=1).drop_duplicates()\n",
    "loc_data = loc_data[(loc_data['latitude'] > 40.5) & (loc_data['latitude'] < 41.1) &\n",
    "                    (loc_data['longitude'] > -74.1) & (loc_data['longitude'] < -73.6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_data['key'] = 1\n",
    "loc_data['key'] = 1\n",
    "result = pd.merge(time_data, loc_data, on='key').drop(['key'], axis=1)\n",
    "result = result[Xtrain.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/var/folders/x3/qp53rvhx52b7gxvpc7dk0gt00000gn/T/kernel-PySpark-d414eddb-389e-437a-a1f7-f7bb7f467b8b/pyspark_runner.py:2: SettingWithCopyWarning: \n",
       "A value is trying to be set on a copy of a slice from a DataFrame.\n",
       "Try using .loc[row_indexer,col_indexer] = value instead\n",
       "\n",
       "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
       "  # Licensed to the Apache Software Foundation (ASF) under one or more\n",
       "/var/folders/x3/qp53rvhx52b7gxvpc7dk0gt00000gn/T/kernel-PySpark-d414eddb-389e-437a-a1f7-f7bb7f467b8b/pyspark_runner.py:3: SettingWithCopyWarning: \n",
       "A value is trying to be set on a copy of a slice from a DataFrame.\n",
       "Try using .loc[row_indexer,col_indexer] = value instead\n",
       "\n",
       "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
       "  # contributor license agreements.  See the NOTICE file distributed with\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = dftaxi[['geohash','day_num','pickups']]\n",
    "yy['latitude'] = yy['geohash'].apply(lambda geo: decodegeo(geo, 0))\n",
    "yy['longitude'] = yy['geohash'].apply(lambda geo: decodegeo(geo, 1))\n",
    "result['pred_pickups'] = np.power(10,reg.predict(result)) - 1\n",
    "result = pd.merge(result, yy, how='left', on=['day_num', 'latitude', 'longitude']).drop(['geohash'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   time_num  time_cos  time_sin   day_num   day_cos   day_sin  weekend   latitude  longitude  pred_pickups  pickups\n",
       "0  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.754471 -74.000473      1.000000      NaN\n",
       "1  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.766830 -73.978500      4.302656      NaN\n",
       "2  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.765457 -73.913956      1.625663      NaN\n",
       "3  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.746231 -73.948288      1.286634      NaN\n",
       "4  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.746231 -74.004593      1.607528      1.0\n",
       "5  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.825882 -73.935928      1.148326      NaN\n",
       "6  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.794296 -73.971634     11.363229     11.0\n",
       "7  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.740738 -73.999100      1.855802      2.0\n",
       "8  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.705032 -74.010086      4.652040      4.0\n",
       "9  0.989583  0.997859 -0.065403  0.141369  0.630773  0.775968        0  40.705032 -74.010086      4.652040      5.0\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = result.drop(['time_cos','day_num','time_sin','day_cos','day_sin','weekend'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('/Users/hongyili/Desktop/big_Data_final/taxi-data-predictions_prec_7_monday.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/var/folders/x3/qp53rvhx52b7gxvpc7dk0gt00000gn/T/kernel-PySpark-d414eddb-389e-437a-a1f7-f7bb7f467b8b/pyspark_runner.py\", line 132, in <module>\n",
       "    compiled_code = compile(final_code, \"<string>\", \"exec\")\n",
       "  File \"<string>\", line 5\n",
       "    result = pd.merge(result, yy, how='left', on=['day_num', 'latitude', 'longitude']), axis=1)\n",
       "                                                                                              ^\n",
       "SyntaxError: invalid syntax\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:140)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:140)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:139)\n",
       "sun.reflect.GeneratedMethodAccessor93.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
       "py4j.Gateway.invoke(Gateway.java:259)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = dftaxi[['geohash','day_num','pickups']]\n",
    "yy['latitude'] = yy['geohash'].apply(lambda geo: decodegeo(geo, 0))\n",
    "yy['longitude'] = yy['geohash'].apply(lambda geo: decodegeo(geo, 1))\n",
    "result['pred_pickups'] = np.power(10,reg.predict(result)) - 1\n",
    "result = pd.merge(result, yy, how='left', on=['day_num', 'latitude', 'longitude']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/var/folders/x3/qp53rvhx52b7gxvpc7dk0gt00000gn/T/kernel-PySpark-d414eddb-389e-437a-a1f7-f7bb7f467b8b/pyspark_runner.py\", line 134, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 4, in <module>\n",
       "  File \"//anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py\", line 649, in predict\n",
       "    X = self._validate_X_predict(X)\n",
       "  File \"//anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py\", line 319, in _validate_X_predict\n",
       "    return self.estimators_[0]._validate_X_predict(X, check_input=True)\n",
       "  File \"//anaconda/lib/python2.7/site-packages/sklearn/tree/tree.py\", line 365, in _validate_X_predict\n",
       "    X = check_array(X, dtype=DTYPE, accept_sparse=\"csr\")\n",
       "  File \"//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py\", line 398, in check_array\n",
       "    _assert_all_finite(array)\n",
       "  File \"//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py\", line 54, in _assert_all_finite\n",
       "    \" or a value too large for %r.\" % X.dtype)\n",
       "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:140)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:140)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:139)\n",
       "sun.reflect.GeneratedMethodAccessor93.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
       "py4j.Gateway.invoke(Gateway.java:259)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = dftaxi[['geohash','day_num','pickups']]\n",
    "yy['latitude'] = yy['geohash'].apply(lambda geo: decodegeo(geo, 0))\n",
    "yy['longitude'] = yy['geohash'].apply(lambda geo: decodegeo(geo, 1))\n",
    "result['pred_pickups'] = np.power(10,reg.predict(result)) - 1\n",
    "result = pd.merge(result, yy, how='left', on=['day_num', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
